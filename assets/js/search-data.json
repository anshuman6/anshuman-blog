{
  
    
        "post0": {
            "title": "Building a basic Neural Network from scratch - Kudzu",
            "content": "In this project, the idea is to write a classifier to differentiate between handwritten digits 3 and 8, from the MNIST database. . I will using a custom built neural network library called Kudzu.This has been developed as a part of KTF/Foundations course of Univ.ai (www.univ.ai). The source code for Kudzu Library and and this notebook can be found on my git-hub profile : www.https://github.com/anshuman6 under the folder project-solution. Any feedback and comments are welcome. Please feel free to email me at: anshuman6@gmail.com . 1. I will be using the MNIST database. . 2. I will be comparing the results of the 4 layer NN to a standard logistic regression . Importing all the necessary libraries . %load_ext autoreload %autoreload 2 . %matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . The following command helps us download MNIST from notebook itself. You can skip this if you already have MNIST. You can also download it via your terminal. . !pip install mnist # Please note, it is commented out for now, you can remove comment if you want to install it . Collecting mnist Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB) Requirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.6.11/x64/lib/python3.6/site-packages (from mnist) (1.19.1) Installing collected packages: mnist Successfully installed mnist-0.2.2 . Preparing the Data . import mnist . train_images = mnist.train_images() train_labels = mnist.train_labels() . train_images.shape, train_labels.shape . ((60000, 28, 28), (60000,)) . test_images = mnist.test_images() test_labels = mnist.test_labels() . test_images.shape, test_labels.shape . ((10000, 28, 28), (10000,)) . image_index = 7776 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . 2 . &lt;matplotlib.image.AxesImage at 0x7fb3f1331b38&gt; . Filter data to get 3 and 8 out . train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . We normalize the pizel values in the 0 to 1 range . X_train = X_train/255. X_test = X_test/255. . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . We reshape the data to flatten the image pixels into a set of features or co-variates: . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . Importing Kudzu library and its functionality . from kudzu.data import Data, Dataloader, Sampler from kudzu.callbacks import AccCallback from kudzu.loss import MSE from kudzu.layer import Affine, Sigmoid from kudzu.model import Model from kudzu.optim import GD from kudzu.train import Learner from kudzu.callbacks import ClfCallback from kudzu.layer import Sigmoid from kudzu.layer import Relu . We are creating a class, just so that we can use it to store our parameters for us . class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 250 config.bs = 50 . Initializing Data . data = Data(X_train, y_train.reshape(-1,1)) loss = MSE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle=True) dl = Dataloader(data, sampler) . Constructing 2 different layers, one for NN and one only for logistic regression. . Creating containers for data to be passed, to calculate accuracies . training_xdata = X_train testing_xdata = X_test training_ydata = y_train.reshape(-1,1) testing_ydata = y_test.reshape(-1,1) . Initializing models and Running training loop: . layers = [Affine(&quot;first&quot;, 784, 100), Relu(&quot;first&quot;), Affine(&quot;second&quot;, 100, 100), Relu(&quot;second&quot;), Affine(&quot;third&quot;, 100, 2), Affine(&quot;final&quot;, 2, 1), Sigmoid(&quot;final&quot;)] model_neural = Model(layers) model_logistic = Model([Affine(&quot;logits&quot;, 784, 1), Sigmoid(&quot;sigmoid&quot;)]) . learner1 = Learner(loss, model_neural, opt, config.num_epochs) acc1 = ClfCallback(learner1, config.bs, training_xdata , testing_xdata, training_ydata, testing_ydata) learner1.set_callbacks([acc1]) . learner1.train_loop(dl) . Epoch 0 Loss 0.2377525667257724 train accuracy is: 0.7318477716574863, test accuracy is 0.7318548387096774 Epoch 10 Loss 0.07444673573438723 train accuracy is: 0.9276414621932899, test accuracy is 0.9334677419354839 Epoch 20 Loss 0.05032075322504384 train accuracy is: 0.9460023368385913, test accuracy is 0.9551411290322581 Epoch 30 Loss 0.041433234776114276 train accuracy is: 0.9550158571190118, test accuracy is 0.9621975806451613 Epoch 40 Loss 0.03660875912903833 train accuracy is: 0.9602737439492572, test accuracy is 0.9647177419354839 Epoch 50 Loss 0.03349998826180057 train accuracy is: 0.9629444166249375, test accuracy is 0.9672379032258065 Epoch 60 Loss 0.03127904035259931 train accuracy is: 0.9650308796528125, test accuracy is 0.96875 Epoch 70 Loss 0.029572826728512403 train accuracy is: 0.9672008012018027, test accuracy is 0.9682459677419355 Epoch 80 Loss 0.028219788627103438 train accuracy is: 0.9688699716241028, test accuracy is 0.9707661290322581 Epoch 90 Loss 0.02710693468402134 train accuracy is: 0.9699549323985979, test accuracy is 0.9702620967741935 Epoch 100 Loss 0.026153181760343123 train accuracy is: 0.970789517609748, test accuracy is 0.969758064516129 Epoch 110 Loss 0.025325793684624457 train accuracy is: 0.971290268736438, test accuracy is 0.9722782258064516 Epoch 120 Loss 0.024581411991316932 train accuracy is: 0.971957936905358, test accuracy is 0.9727822580645161 Epoch 130 Loss 0.023903720111409003 train accuracy is: 0.9728759806376232, test accuracy is 0.9737903225806451 Epoch 140 Loss 0.023312391039097012 train accuracy is: 0.9732098147220831, test accuracy is 0.9742943548387096 Epoch 150 Loss 0.02275208077345371 train accuracy is: 0.9737105658487731, test accuracy is 0.9753024193548387 Epoch 160 Loss 0.02222810643044528 train accuracy is: 0.9747120681021533, test accuracy is 0.9747983870967742 Epoch 170 Loss 0.021738062895438004 train accuracy is: 0.9755466533133033, test accuracy is 0.9742943548387096 Epoch 180 Loss 0.02127050849797675 train accuracy is: 0.9762143214822233, test accuracy is 0.9742943548387096 Epoch 190 Loss 0.02082461976626869 train accuracy is: 0.9769654481722584, test accuracy is 0.9747983870967742 Epoch 200 Loss 0.02040637381957419 train accuracy is: 0.9773827407778334, test accuracy is 0.9747983870967742 Epoch 210 Loss 0.02000001731422593 train accuracy is: 0.9780504089467534, test accuracy is 0.9753024193548387 Epoch 220 Loss 0.019608856374723812 train accuracy is: 0.9786346185945585, test accuracy is 0.9758064516129032 Epoch 230 Loss 0.019230506069702183 train accuracy is: 0.9786346185945585, test accuracy is 0.9758064516129032 Epoch 240 Loss 0.01885726179917066 train accuracy is: 0.9790519112001336, test accuracy is 0.9758064516129032 . 0.013637334990772736 . Now running only the logistic regression based classification to compare results with NN . learner2 = Learner(loss, model_logistic, opt, config.num_epochs) acc2 = ClfCallback(learner2, config.bs, training_xdata , testing_xdata, training_ydata, testing_ydata) learner2.set_callbacks([acc2]) . learner2.train_loop(dl) . Epoch 0 Loss 0.23208763961458434 train accuracy is: 0.7461191787681523, test accuracy is 0.7373991935483871 Epoch 10 Loss 0.09745028719429598 train accuracy is: 0.9204640293773995, test accuracy is 0.9319556451612904 Epoch 20 Loss 0.07610863415215356 train accuracy is: 0.9305625104323152, test accuracy is 0.9405241935483871 Epoch 30 Loss 0.06624408735031058 train accuracy is: 0.9370722750792856, test accuracy is 0.9450604838709677 Epoch 40 Loss 0.060221989612594354 train accuracy is: 0.942580537472876, test accuracy is 0.9516129032258065 Epoch 50 Loss 0.05605836695840553 train accuracy is: 0.9462527124019362, test accuracy is 0.954133064516129 Epoch 60 Loss 0.05296242228489321 train accuracy is: 0.9480053413453514, test accuracy is 0.9561491935483871 Epoch 70 Loss 0.05054985597614866 train accuracy is: 0.9496745117676515, test accuracy is 0.9581653225806451 Epoch 80 Loss 0.04860198282805711 train accuracy is: 0.9517609747955266, test accuracy is 0.9596774193548387 Epoch 90 Loss 0.04698681406301292 train accuracy is: 0.9529293940911367, test accuracy is 0.9611895161290323 Epoch 100 Loss 0.045619526527634235 train accuracy is: 0.9539308963445168, test accuracy is 0.9616935483870968 Epoch 110 Loss 0.04444350389361706 train accuracy is: 0.9546820230345519, test accuracy is 0.9632056451612904 Epoch 120 Loss 0.04341897705373772 train accuracy is: 0.9554331497245869, test accuracy is 0.9637096774193549 Epoch 130 Loss 0.04251287048502711 train accuracy is: 0.9561842764146219, test accuracy is 0.9637096774193549 Epoch 140 Loss 0.0417085434048568 train accuracy is: 0.9567684860624269, test accuracy is 0.9647177419354839 Epoch 150 Loss 0.040985138298143695 train accuracy is: 0.957686529794692, test accuracy is 0.9652217741935484 Epoch 160 Loss 0.04032924633981336 train accuracy is: 0.9581872809213821, test accuracy is 0.9652217741935484 Epoch 170 Loss 0.03973306781958908 train accuracy is: 0.9584376564847271, test accuracy is 0.9647177419354839 Epoch 180 Loss 0.039186062423336235 train accuracy is: 0.9587714905691871, test accuracy is 0.9647177419354839 Epoch 190 Loss 0.038683394448199786 train accuracy is: 0.9589384076114171, test accuracy is 0.9647177419354839 Epoch 200 Loss 0.03821860383058323 train accuracy is: 0.9594391587381071, test accuracy is 0.9647177419354839 Epoch 210 Loss 0.03778634923922105 train accuracy is: 0.9596895343014522, test accuracy is 0.9647177419354839 Epoch 220 Loss 0.03738427238209248 train accuracy is: 0.9601068269070272, test accuracy is 0.9657258064516129 Epoch 230 Loss 0.03700813864476025 train accuracy is: 0.9607744950759473, test accuracy is 0.9657258064516129 Epoch 240 Loss 0.03665466273838694 train accuracy is: 0.9609414121181773, test accuracy is 0.9657258064516129 . 0.06839981408424851 . Comparing results of NN and LR . plt.figure(figsize=(8,5)) plt.plot(acc1.val_accuracies, &#39;g-&#39;, label = &quot;Val Accuracies - NN&quot;) plt.plot(acc1.accuracies, &#39;r-&#39;, label = &quot;Accuracies - NN&quot;) plt.plot(acc2.val_accuracies, &#39;b-&#39;, label = &quot;Val Accuracies - Logistic Reg&quot;) plt.plot(acc2.accuracies, &#39;k-&#39;, label = &quot;Accuracies - Logistic Reg&quot;) plt.ylim(0.8,1) ## for a more spread out view plt.legend() . &lt;matplotlib.legend.Legend at 0x7fb3f0f132b0&gt; . Clearly NN has a better accuracy over LR; NN is overfitting - Validation accuracy (green) has dropped below training accuracy (orange), also they are diverging. This problem is not seen in LR . Now we will be moving through the network, till the output of the second last affine where we get a 2 dimensional output. We will be plotting this 2d output and probability contours . model_new = Model(layers[:-2]) . plot_testing = model_new(testing_xdata) . Plotting the scatter plot of points and color coding by class . plt.figure(figsize=(8,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], alpha = 0.1, c = y_test.ravel()); . Plotting probability contours . model_prob = Model(layers[-2:]) ## picking only last two layers to get probability. That is affine followed by sigmoid . xgrid = np.linspace(-4, 1, 100) ## Adjust these values based on above chart, roughly -4 to 1 ygrid = np.linspace(-7.5, 7.5, 100) ## Adjust these values based on above chart, roughly -7.5, 7.5 xg, yg = np.meshgrid(xgrid, ygrid) # xg and yg are now both 100X100, lets convert them to single arrays xg_interim = np.ravel(xg) yg_interim = np.ravel(yg) ## xg_interim, yg_interim are now arrays of len 10000, now we will stack them and then transpose to get desired shape of n rows, 2 columns X_interim = np.vstack((xg_interim, yg_interim)) ## Please note vstack takes in a tuple X = X_interim.T ## We want a shape of n rows and 2 columns to be able to feed this to last affine ## This last affine takes only two columns, hence the above transformation probability_contour = model_prob(X).reshape(100,100) ## to make it consistent with xg, yg . plt.figure(figsize=(8,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], alpha = 0.1, c = y_test.ravel()) contours = plt.contour(xg,yg,probability_contour) plt.clabel(contours, inline = True ); .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/07/NN-new.html",
            "relUrl": "/2020/08/07/NN-new.html",
            "date": " • Aug 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Dynamic Covid-19 Tracker",
            "content": "Hello!, This is a dynamic version of the dashboard, it updates once daily! . Data source is https://www.covid19india.org/ . #collapse from datetime import datetime import pandas as pd import numpy as np import requests import json import matplotlib.pyplot as plt import matplotlib.dates as mdates import matplotlib as mpl from IPython.core.display import display,HTML import pytz %matplotlib inline dynamic_df = pd.read_csv(&quot;https://api.covid19india.org/csv/latest/state_wise_daily.csv&quot;) dynamic_df.head() ddf = dynamic_df[(dynamic_df.Status == &quot;Confirmed&quot;)] ddf ddf1 = ddf.drop(columns = [&quot;Status&quot;]) ddf2 = dynamic_df[(dynamic_df.Status == &quot;Deceased&quot;)] ddf2 = ddf2.drop(columns = [&quot;Status&quot;]) ddf1[&quot;Date&quot;] = ddf1[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) update = dynamic_df.iloc[-1,0] cases = ddf1.TT.sum() new = ddf1.iloc[-1,1] deaths = ddf2.TT.sum() dnew = ddf2.iloc[-1,1] overview = &#39;&#39;&#39; &lt;!-- ####### HTML!! #########--&gt; &lt;h1 style=&quot;color: #5e9ca0; text-align: center;&quot;&gt;India&lt;/h1&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Last update: &lt;strong&gt;{update}&lt;/strong&gt;&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed cases:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{cases} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed deaths:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{deaths} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{dnew}&lt;/span&gt;)&lt;/p&gt; &#39;&#39;&#39; html = HTML(overview.format(update=update, cases=cases,new=new,deaths=deaths,dnew=dnew)) display(html) . . India . Last update: 31-Oct-21 . Confirmed cases: . 34284758 (+2021-10-31) . Confirmed deaths: . 458475 (+2021-10-31) . This dashboard is last updated at (IST) = 2022-05-17 15:49 . #collapse ch_total = ddf1.CH.sum() ch_new = ddf1.iloc[-1,7] mh_total = ddf1.MH.sum() mh_new = ddf1.iloc[-1,23] dl_total = ddf1.DL.sum() dl_new = ddf1.iloc[-1,11] firstdata = &#39;&#39;&#39; &lt;!-- ####### HTML!! #########--&gt; &lt;p style=&quot;text-align: center;font-size:18px;&quot;&gt;--Important Places--&lt;/p&gt; &lt;p style=&quot;text-align: left;font-size:18px;&quot;&gt;Total Cases (&lt;span style=&quot;color: #ff0000;&quot;&gt;New Cases&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: left;font-size:18px;&quot;&gt;Chandigarh (Hometown): {ch_total} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{ch_new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: left;font-size:18px;&quot;&gt;Delhi (Second Home): {dl_total} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{dl_new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: left;font-size:18px;&quot;&gt;Maharashtra (Just Because..): {mh_total} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{mh_new}&lt;/span&gt;)&lt;/p&gt; &#39;&#39;&#39; html = HTML(firstdata.format(ch_total=ch_total, ch_new = ch_new, mh_total = mh_total, mh_new = mh_new, dl_total = dl_total, dl_new = dl_new )) display(html) . . --Important Places-- . Total Cases (New Cases) . Chandigarh (Hometown): 65351 (+8) . Delhi (Second Home): 1439870 (+0) . Maharashtra (Just Because..): 6611078 (+16) . #collapse n = 10 st = [&quot;TT&quot;, &quot;MH&quot;, &quot;TN&quot;, &quot;DL&quot;, &quot;KA&quot;, &quot;UP&quot;, &quot;BR&quot;, &quot;WB&quot;, &quot;TG&quot;, &quot;CH&quot;] st_name = [&quot;Daily Count for India&quot;, &quot;Maharashta&quot;, &quot;Tamil Nadu&quot;, &quot;Delhi&quot;, &quot;Karnataka&quot;, &quot;Uttar Pradesh&quot;, &quot;Bihar&quot;, &quot;West Bengal&quot;, &quot;Telangana&quot;, &quot;Chandigarh (My hometown)&quot;] ax = [] fig = plt.figure(figsize = (16,30)) gs = fig.add_gridspec(n, 3) for i in range(n): ax1 = fig.add_subplot(gs[i, :]) ax1.bar(ddf1.Date,ddf1[st[i]],alpha=0.3,color=&#39;#007acc&#39;) ax1.plot(ddf1.Date,ddf1[st[i]] , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax1.xaxis.set_major_locator(mdates.WeekdayLocator()) ax1.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax1.text(0.02, 0.5,st_name[i], transform = ax1.transAxes, fontsize=25) ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) . .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/06/dynamic.html",
            "relUrl": "/2020/08/06/dynamic.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic gradient descent implementation in python",
            "content": "# Import all important packages . import pandas as pd import numpy as np import matplotlib.pyplot as plt . #Define our main function . def f_func(x): return ((x*x) + 3) . # Lets try to visualize the shape of our main function # So let us generate random x values, which we will feed into f, to generate f values for plotting. . x_vals = np.linspace(-5,5,100) . # Now plotting a graph between x values (defined as x_vals and generating as a linear set of values from -5 to 5) . plt.plot(x_vals, f_func(x_vals)) . [&lt;matplotlib.lines.Line2D at 0x7f00e956b5f8&gt;] . # Visually we can see that global minima seems to be at 0, lets verify this by grad descent also . # Now let us calculate and define a function for the derivate of the main function which will then be used in derivative descent. # Let us call this f_prime . def f_prime(x): return ((2*x)) . # Now let us define the derivate descent function . def derivative_descent(start,f_prime, lr): vals = [] current = start while (current &gt; 0.01): old = current move = lr*f_prime(old) current = old - move vals.append(current) return vals . output = derivative_descent(3, f_prime, 0.001) . plt.plot(output) . [&lt;matplotlib.lines.Line2D at 0x7f00e9467cc0&gt;] . # The above figure shows how we started from x = 5 and progressively dropped to about 0 in 3000 iterations . # Now lets use the output values of these x and put into main function to visualize . plt.plot(x_vals, f_func(x_vals)) plt.plot(output, [f_func(elem) for elem in output], &#39;rx&#39;, alpha = 0.1) . [&lt;matplotlib.lines.Line2D at 0x7f00e947e048&gt;] . # As we can see above.. we started at x=3... and dropped down, following the red path to x=0 &gt; which is our minima .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/05/Basic_Gradient_Descent.html",
            "relUrl": "/2020/08/05/Basic_Gradient_Descent.html",
            "date": " • Aug 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "The devil is in the details (Bayes Theorem and Covid-19)",
            "content": "Serological surveys refer to conducting antibody tests on populations to ascertain how many people already have antibodies against coronavirus (Covid-19). This means at some point they have already contracted coronavirus and may/may-not have shown symptoms. In either situation, presence of antibodies can suggest some form of immunity against future infections from coronavirus (to what extent though, is still to be scientifically proven). . Serological surveys can give useful information to scientists about the spead patterns of the disease. One school of thought also suggests that people with antibodies &quot;might&quot; be at lower risk to return back to workforce. . Serological tests are different from RT-PCR tests (reverse transcriptase polymerase chain reaction), you might them as the &#39;swab&#39; tests. Swab tests are very different from antibody tests. Swab test tells you if the host has the virus at present, antibody test tells you the hosts response to the virus - by checking if host as antibodies against the test, giving indication that the host might have recently or in the near past, might have contracted coronavirus. . When it comes to any form of testing, there are three important parameters to understand: Sensitivity and Specificity and a &#39;Third One&#39;, which we will come to later. . Sensitivity basically tells you, what are the chances that if you test positive, you actually have the disease/antibodies (depending on what you are testing for). (Think of it as True positive Rate). You will usually see tests claiming different sensitivity values - 98%, 99% etc. What this means is that if you get tested positive, there is a 98% chance that you have the disease/antibodies. What this also means is that 2% of the times, you will get a &#39;false positive&#39; - that is, you get a positive, although you dont have the diesease/antibodies. . This can be a little dangerous, if you fall in that 2% zone of an antibody test, you might think that you already have the antibodies and are at a lower risk, however, this might not be true and you might be putting yourself at risk if you go out and expose yourself more. . Specificity on the other hand tells you, what are the chances that if you test negative, you actually do not have the disease/antibodies (depending on what you are testing for). (Think of it as True negative Rate). If specificity is 98%, 2% of people will test negative, but will have the disease/antibodies. So if you get a test done, and you tested negative, there is still chance that you might have the disease (or antibodies)! So 2% of the times you have the possibility of getting a &#39;false negative&#39;. . Now coming to the third parameter. Theoretically, If you are able to somehow get your hands on a test which claims high sensitivity and specificty, can you rule out the possibility of false positives and false negatives? Unfortunately not. . Unless you completely understand, how the manufacturer of the test calculated these claimed specificty and sensitivity values, you cannot be sure. . Results of tests, in the real world, vary from person to person. Someone might have high number of antibodies and the test might catch it easily, someone might not and the test might fail. These claimed values need to be tested from large samples of populations (sampled correctly keeping various randomizing factors in mind), before these claimed values can be trusted. However, unfortunately, not a lot of test manufacturers disclose actual details of how these claimed values of specificity and sensitivity are calculated. . Not to go very deep, but there is one more factor to take into consideration - it is the disease prevalence rate. How widespread is the disease in the general population. Do a lot of people have it? If the prevalence is low, the chances of getting false positives are higher. You might test positive, but you might not have the disease. If prevalence is high, you might test negative, but still have the disease! . So in conclusion, the devil is in the details. Diagnostic science is not 100 percent foolproof, but with the combinations of different modalities, you can be more certain. . . Below is a python snippet and a visualization showcasing the point about prevalence and its impact on false positives. . So if you are in a place where a disease is not prevalent, and you test positive for it. There are chances that it might be a false positive and you might need to test again using other modalities to confirm and re-confirm. This is if you are towards the left side of the chart below. . import numpy as np import matplotlib.pyplot as plt s1 = .90 # P(T+|D+) s2 = 0.70 # P(T+|D+) spec = .98 # P(T-|D-) dplus = np.arange(0,1,0.01) pdneg = 1 - dplus # Using magical bayes theorem equations - # at 90% dplustplus1 = (s1 * dplus) / ((s1 * dplus) + ((1-spec)*pdneg)) # at 70% dplustplus2 = (s2 * dplus) / ((s2* dplus) + ((1-spec)*pdneg)) plt.figure(figsize = (10,8)) plt.plot(dplus,dplustplus1, &#39;g&#39;, dplus,dplustplus2,&#39;b&#39;) plt.xlabel(&quot;Prevalence&quot;,fontsize = 17) plt.ylabel(&quot;If tested positive, chance that you actually have the disease&quot;, fontsize = 15) plt.title(&quot;Green - 90% sensitivity, Blue - 70% Sensitivity&quot;, fontsize = 17) plt.show() .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/01/surveys.html",
            "relUrl": "/2020/08/01/surveys.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Quick PDF, CDF Implementation in code",
            "content": "Just some quick code to understand PDF and CDF from a code/viz perspective, for when math equations can look scary . # Import all the nice packages import numpy as np import math import matplotlib.pyplot as plt import scipy.integrate as integrate import scipy.stats . # Decide on a mean and sigma for the gaussian dist. mean = 0 sigma = 2.5 # Generate random data to plot the gaussian x = np.arange(-10,10,0.1) . y_values = scipy.stats.norm(mean, sigma) y_values_pdf = y_values.pdf(x) y_values_cdf = y_values.cdf(x) # Nice function to calculate integral area = integrate.quad(lambda x: y_values.pdf(x), -10, 0) area[0] area_str = str(round(area[0], 2)) . # Matplotlib magic plt.figure(figsize = (15,6)) plt.subplot(1,2,1) plt.plot(x, y_values_pdf) plt.text(-10, 0.1, &quot;Area in highlighted region = probability of getting 0 or less = {}&quot;.format(area_str)) plt.fill_between(x, y_values_pdf, where = (x&lt;0), color = &quot;g&quot;) plt.title(&quot;Probability Density Function&quot;) plt.subplot(1,2,2) plt.plot(x, y_values_cdf) plt.axhline(y=0.5) plt.axvline(x=0) plt.title(&quot;Cumulative Density Function&quot;) plt.show() . Area under PDF is the probability. Any point on the CDF is the the probability of getting that value OR less. . Example - The shaded green region is the probability of getting 0 or less from PDF &gt; 0.5 in this case (half of fig is shaded) . If you draw a vertical line at 0, it intersects the CDF at 0.5 -&gt; shows probability of getting 0 OR less is 0.5 .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/01/gaussian.html",
            "relUrl": "/2020/08/01/gaussian.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Covid-19 Tracker",
            "content": "Hello, welcome to my dashboard. I have created it using matplotlib. This is static. You can collapse cells (&quot;Show Code&quot;) if you want to review the code . #collapse import pandas as pd import numpy as np import requests import json import matplotlib.pyplot as plt import matplotlib.dates as mdates import matplotlib as mpl from IPython.core.display import display,HTML %matplotlib inline dft_cases = pd.read_csv(&#39;data/SnapshotCases-28-July.csv&#39;) dft_deaths = pd.read_csv(&#39;data/SnapshotDeaths-28-July.csv&#39;) dft_cases[&quot;dt_today&quot;] = dft_cases[&quot;28-Jul-20&quot;] dft_cases[&quot;dt_yday&quot;] = dft_cases[&quot;27-Jul-20&quot;] dft_deaths[&quot;dt_today&quot;] = dft_deaths[&quot;28-Jul-20&quot;] dft_deaths[&quot;dt_yday&quot;] = dft_deaths[&quot;27-Jul-20&quot;] dfc_cases = dft_cases.groupby(&#39;states&#39;)[&quot;dt_today&quot;].sum() dfc_deaths = dft_deaths.groupby(&#39;states&#39;)[&quot;dt_today&quot;].sum() dfp_cases = dft_cases.groupby(&#39;states&#39;)[&quot;dt_yday&quot;].sum() dfp_deaths = dft_deaths.groupby(&#39;states&#39;)[&quot;dt_yday&quot;].sum() df_dfc_cases = pd.DataFrame(dfc_cases).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_today&quot;: &quot;Cases&quot;}) df_dfc_deaths = pd.DataFrame(dfc_deaths).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_today&quot;: &quot;Deaths&quot;}) df_dfp_cases = pd.DataFrame(dfp_cases).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_yday&quot;: &quot;PCases&quot;}) df_dfp_deaths = pd.DataFrame(dfp_deaths).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_yday&quot;: &quot;PDeaths&quot;}) df_table = pd.merge(df_dfc_cases,df_dfp_cases, how=&#39;outer&#39;) df_table = pd.merge(df_table,df_dfc_deaths, how=&#39;outer&#39;) df_table = pd.merge(df_table,df_dfp_deaths, how=&#39;outer&#39;) for c in &#39;Cases, Deaths&#39;.split(&#39;, &#39;): df_table[f&#39;{c} (+)&#39;] = (df_table[c] - df_table[f&#39;P{c}&#39;]).clip(0) df_table[&#39;Fatality Rate&#39;] = (100* df_table[&#39;Deaths&#39;]/ df_table[&#39;Cases&#39;]).round(2) df_table.sort_values(by = [&#39;Cases&#39;,&#39;Deaths&#39;], ascending = [False, False], inplace = True) df_table.reset_index(drop=True, inplace = True) summary = {&quot;updated&quot;:&quot;28th July, 2020&quot;, &quot;since&quot;:&quot;27th July, 2020&quot;} for col in df_table.columns: if col != &quot;states&quot; and col!= &quot;Fatality Rate&quot;: summary[col]= df_table[col].sum() update = summary[&#39;updated&#39;] cases = summary[&#39;Cases&#39;] new = summary[&#39;Cases (+)&#39;] deaths = summary[&#39;Deaths&#39;] dnew = summary[&#39;Deaths (+)&#39;] overview = &#39;&#39;&#39; &lt;!-- ####### HTML!! #########--&gt; &lt;h1 style=&quot;color: #5e9ca0; text-align: center;&quot;&gt;India&lt;/h1&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Last update: &lt;strong&gt;{update}&lt;/strong&gt;&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed cases:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{cases} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed deaths:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{deaths} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{dnew}&lt;/span&gt;)&lt;/p&gt; &#39;&#39;&#39; html = HTML(overview.format(update=update, cases=cases,new=new,deaths=deaths,dnew=dnew)) display(html) . . India . Last update: 28th July, 2020 . Confirmed cases: . 1514800 (+49001) . Confirmed deaths: . 34121 (+770) . #collapse dt_cols = list(dft_cases.columns[1:]) dft_ct_new_cases = dft_cases.groupby(&#39;states&#39;)[dt_cols].sum().diff(axis=1).fillna(0).astype(int) dft_ct_new_cases.sort_values(by = &#39;28-Jul-20&#39;, ascending = False,inplace = True) df = dft_ct_new_cases.copy() df.loc[&#39;Total&#39;] = df.sum() df.drop([&#39;dt_today&#39;, &#39;dt_yday&#39;], axis=1, inplace = True) n = 5 ef = df.loc[&#39;Total&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax = [] fig = plt.figure(figsize = (16,20)) gs = fig.add_gridspec(n+2, 3) # gs = fig.add_gridspec(2, 3) ax1 = fig.add_subplot(gs[0, :]) ef = df.loc[&#39;Total&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax1.bar(ef.date,ef.Total,alpha=0.3,color=&#39;#007acc&#39;) ax1.plot(ef.date,ef.Total , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax1.xaxis.set_major_locator(mdates.WeekdayLocator()) ax1.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax1.text(0.02, 0.5,&#39;India daily case count&#39;, transform = ax1.transAxes, fontsize=25); ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax2 = fig.add_subplot(gs[1,0]) ef = df.loc[&#39;Maharashtra&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax2.bar(ef.date, ef.Maharashtra,color = &#39;#007acc&#39;,alpha=0.5) ax2.xaxis.set_major_locator(mdates.WeekdayLocator()) ax2.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax2.set_xticks(ax2.get_xticks()[::3]) maxyval = ef.Maharashtra.max() ax2.set_ylim([0,maxyval]) ax2.text(0.05, 0.5,&#39;Maharashtra&#39;, transform = ax2.transAxes, fontsize=20); ax2.spines[&#39;right&#39;].set_visible(False) ax2.spines[&#39;top&#39;].set_visible(False) ax3 = fig.add_subplot(gs[1,1]) ef = df.loc[&#39;Tamil Nadu&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax3.bar(ef.date, ef[&#39;Tamil Nadu&#39;],color = &#39;#007acc&#39;,alpha=0.5,) ax3.xaxis.set_major_locator(mdates.WeekdayLocator()) ax3.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax3.set_xticks(ax3.get_xticks()[::3]) ax3.text(0.05, 0.5,&#39;Tamil Nadu&#39;, transform = ax3.transAxes, fontsize=20); ax3.spines[&#39;right&#39;].set_visible(False) ax3.spines[&#39;top&#39;].set_visible(False) ax4 = fig.add_subplot(gs[1,2]) ef = df.loc[&#39;Delhi&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax4.bar(ef.date, ef.Delhi,color = &#39;#007acc&#39;,alpha=0.5) ax4.set_xticks([]) ax4.xaxis.set_major_locator(mdates.WeekdayLocator()) ax4.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax4.set_xticks(ax4.get_xticks()[::3]) ax4.spines[&#39;right&#39;].set_visible(False) ax4.spines[&#39;top&#39;].set_visible(False) ax4.text(0.05, 0.5,&#39;Delhi&#39;, transform = ax4.transAxes, fontsize=20) for i in range(n): ax.append(fig.add_subplot(gs[i+2,:])) ef = df.iloc[i+3].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax[i].bar(ef.date,ef.iloc[:,-1],color = &#39;#007acc&#39;,alpha=0.3) ax[i].plot(ef.date,ef.iloc[:,-1],marker=&#39;o&#39;,color=&#39;#007acc&#39;) ax[i].text(0.02,0.5,f&#39;{ef.columns.values[-1]}&#39;,transform = ax[i].transAxes, fontsize = 20); ax[i].xaxis.set_major_locator(mdates.WeekdayLocator()) ax[i].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax[i].set_ylim([0,7000]) ax[i].spines[&#39;right&#39;].set_visible(False) ax[i].spines[&#39;top&#39;].set_visible(False) plt.tight_layout() . . #collapse print(df_table.to_string(index=False)) . . states Cases PCases Deaths PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 383723 14164 13882 7717 282 3.62 Tamil Nadu 227688 220716 3659 3571 6972 88 1.61 Delhi 132275 131219 3881 3853 1056 28 2.93 Andhra Pradesh 110297 102349 1148 1090 7948 58 1.04 Karnataka 107001 101465 2064 1962 5536 102 1.93 Uttar Pradesh 73951 70493 1497 1456 3458 41 2.02 West Bengal 62964 60830 1449 1411 2134 38 2.30 Gujarat 57982 56874 2372 2348 1108 24 4.09 Telangana 57142 55532 480 471 1610 9 0.84 Bihar 43591 41111 269 255 2480 14 0.62 Rajasthan 38636 37564 644 633 1072 11 1.67 Assam 34846 33475 92 90 1371 2 0.26 Haryana 32876 32127 406 397 749 9 1.23 Madhya Pradesh 29217 28589 831 821 628 10 2.84 Orissa 28107 26892 189 181 1215 8 0.67 Kerala 20895 19728 68 64 1167 4 0.33 Jammu and Kashmir 18879 18390 333 321 489 12 1.76 Punjab 14378 13769 336 318 609 18 2.34 Jharkhand 9563 8803 94 90 760 4 0.98 Goa 5287 5119 36 36 168 0 0.68 Tripura 4287 4066 21 17 221 4 0.49 Pondicherry 3013 2874 47 43 139 4 1.56 Himachal Pradesh 2330 2270 13 13 60 0 0.56 Manipur 2317 2286 0 0 31 0 0.00 Nagaland 1460 1385 4 5 75 0 0.27 Arunachal Pradesh 1330 1239 3 3 91 0 0.23 Chandigarh 934 910 14 14 24 0 1.50 Meghalaya 779 738 5 5 41 0 0.64 Sikkim 592 568 1 1 24 0 0.17 Mizoram 384 361 0 0 23 0 0.00 Andaman and Nicobar Islands 359 334 1 1 25 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/01/anshuman2.html",
            "relUrl": "/2020/08/01/anshuman2.html",
            "date": " • Aug 1, 2020"
        }
        
    
  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page7": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://anshuman6.github.io/anshuman-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}